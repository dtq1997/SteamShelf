"""æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ â€” ç¬”è®°è¯»å†™ã€å¸¸é‡ã€AI ç¬”è®°è¯†åˆ«å·¥å…·å‡½æ•°"""

import hashlib
import json
import os
import random
import re
import time
from datetime import datetime

from cloud_uploader import SteamCloudUploader
from utils import sanitize_filename


NOTES_APPID = "2371090"

# AI ç”Ÿæˆç¬”è®°çš„å›ºå®šå‰ç¼€æ ‡å¿— â€” ç”¨äºè¯†åˆ«å“ªäº›ç¬”è®°æ˜¯ AI å¤„ç†è¿‡çš„
AI_NOTE_PREFIX = "ğŸ¤–AI:"
# æ—§ç‰ˆå‰ç¼€å…³é”®è¯ï¼ˆv2.x ä½¿ç”¨ï¼‰ï¼Œä»éœ€è¯†åˆ«
AI_NOTE_LEGACY_KEYWORD = "ä»¥ä¸‹å†…å®¹ç”±"

def is_ai_note(note: dict) -> bool:
    """æ£€æµ‹ä¸€æ¡ç¬”è®°æ˜¯å¦ä¸º AI ç”Ÿæˆ
    æ ¸å¿ƒé€»è¾‘ï¼šæ ‡é¢˜ä¸­åŒ…å«ã€Œä»¥ä¸‹å†…å®¹ç”±...ç”Ÿæˆã€å³ä¸º AI ç¬”è®°ã€‚
    ä¹Ÿå…¼å®¹æ–°ç‰ˆ ğŸ¤–AI: å‰ç¼€ã€‚
    """
    title = note.get("title", "")
    if not title:
        return False
    # æœ€å¯é çš„æ–¹å¼ï¼šåªè¦æ ‡é¢˜é‡Œå‡ºç°"ä»¥ä¸‹å†…å®¹ç”±"å°±æ˜¯ AI ç¬”è®°
    if AI_NOTE_LEGACY_KEYWORD in title and "ç”Ÿæˆ" in title:
        return True
    # æ–°ç‰ˆå‰ç¼€ï¼ˆå»æ‰å˜ä½“é€‰æ‹©ç¬¦ååŒ¹é…ï¼‰
    clean = title.replace('\ufe0e', '').replace('\ufe0f', '')
    if clean.startswith("ğŸ¤–AI:"):
        return True
    return False

def extract_ai_model_from_note(note: dict) -> str:
    """ä» AI ç¬”è®°æ ‡é¢˜ä¸­æå–æ¨¡å‹åã€‚
    æ‰¾ã€Œä»¥ä¸‹å†…å®¹ç”± XXX ç”Ÿæˆã€ä¸­çš„ XXXï¼Œè¿™å°±æ˜¯æ¨¡å‹åã€‚
    """
    title = note.get("title", "")
    if not title:
        return ""
    m = re.search(r'ä»¥ä¸‹å†…å®¹ç”±\s*(.+?)\s*ç”Ÿæˆ', title)
    return m.group(1).strip() if m else ""

def extract_ai_confidence_from_note(note: dict) -> str:
    """ä» AI ç¬”è®°æ ‡é¢˜ä¸­æå–ç¡®ä¿¡ç¨‹åº¦ã€‚
    æ‰¾ã€Œç¡®ä¿¡ç¨‹åº¦ï¼šXã€ä¸­çš„ Xï¼ˆå¾ˆé«˜/è¾ƒé«˜/ä¸­ç­‰/è¾ƒä½/å¾ˆä½ï¼‰ã€‚
    """
    title = note.get("title", "")
    if not title:
        return ""
    m = re.search(r'ç¡®ä¿¡ç¨‹åº¦[ï¼š:]\s*(å¾ˆé«˜|è¾ƒé«˜|ä¸­ç­‰|è¾ƒä½|å¾ˆä½)', title)
    return m.group(1) if m else ""

def extract_ai_info_volume_from_note(note: dict) -> str:
    """ä» AI ç¬”è®°æ ‡é¢˜ä¸­æå–ä¿¡æ¯é‡ç­‰çº§ã€‚
    æ‰¾ã€Œç›¸å…³ä¿¡æ¯é‡ï¼šXã€ä¸­çš„ Xï¼ˆç›¸å½“å¤š/è¾ƒå¤š/ä¸­ç­‰/è¾ƒå°‘/ç›¸å½“å°‘ï¼‰ã€‚
    v5.7+ æ–°å¢ï¼Œæ—§ç‰ˆç¬”è®°è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
    """
    title = note.get("title", "")
    if not title:
        return ""
    m = re.search(r'ç›¸å…³ä¿¡æ¯é‡[ï¼š:]\s*(ç›¸å½“å¤š|è¾ƒå¤š|ä¸­ç­‰|è¾ƒå°‘|ç›¸å½“å°‘)', title)
    return m.group(1) if m else ""

def extract_ai_info_source_from_note(note: dict) -> str:
    """ä» AI ç¬”è®°æ ‡é¢˜ä¸­æå–ä¿¡æ¯æ¥æºç±»å‹ã€‚
    è¿”å› "web" (è”ç½‘æ£€ç´¢), "local" (è®­ç»ƒæ•°æ®ä¸Steamè¯„æµ‹), æˆ– "" (æ—§ç‰ˆç¬”è®°)ã€‚
    """
    title = note.get("title", "")
    if not title:
        return ""
    if INFO_SOURCE_WEB in title or "è”ç½‘æ£€ç´¢" in title:
        return "web"
    if INFO_SOURCE_LOCAL in title or "è®­ç»ƒæ•°æ®" in title:
        return "local"
    return ""

def extract_ai_quality_from_note(note: dict) -> str:
    """ä» AI ç¬”è®°æ ‡é¢˜ä¸­æå–æ¸¸æˆæ€»ä½“è´¨é‡è¯„ä¼°ã€‚
    æ‰¾ã€Œæ¸¸æˆæ€»ä½“è´¨é‡ï¼šXã€æˆ–æ—§ç‰ˆã€Œæ€»ä½“è´¨é‡ï¼šXã€ä¸­çš„ Xï¼ˆç›¸å½“å¥½/è¾ƒå¥½/ä¸­ç­‰/è¾ƒå·®/ç›¸å½“å·®ï¼‰ã€‚
    v5.9+ æ–°å¢ï¼Œæ—§ç‰ˆç¬”è®°è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
    """
    title = note.get("title", "")
    if not title:
        return ""
    m = re.search(r'(?:æ¸¸æˆ)?æ€»ä½“è´¨é‡[ï¼š:]\s*(ç›¸å½“å¥½|è¾ƒå¥½|ä¸­ç­‰|è¾ƒå·®|ç›¸å½“å·®)', title)
    return m.group(1) if m else ""

def is_insufficient_info_note(note: dict) -> bool:
    """æ£€æµ‹æ˜¯å¦ä¸º"ä¿¡æ¯è¿‡å°‘"æ ‡æ³¨æ€§ç¬”è®°ã€‚"""
    title = note.get("title", "")
    return INSUFFICIENT_INFO_MARKER in title


# AI ç¡®ä¿¡åº¦å¯¹åº” emojiï¼ˆç”¨äºåˆ—è¡¨æ˜¾ç¤ºï¼Œç›´è§‚è¡¨ç¤º AI è‡ªè¯„å¯é ç¨‹åº¦ï¼‰
CONFIDENCE_EMOJI = {
    "å¾ˆé«˜": "ğŸŸ¢",
    "è¾ƒé«˜": "ğŸ”µ",
    "ä¸­ç­‰": "ğŸŸ¡",
    "è¾ƒä½": "ğŸŸ ",
    "å¾ˆä½": "ğŸ”´",
}

# ä¿¡æ¯é‡ç­‰çº§å¯¹åº” emojiï¼ˆç”¨äºæ ‡æ³¨å‚è€ƒä¿¡æ¯çš„å……è¶³ç¨‹åº¦ï¼‰
INFO_VOLUME_EMOJI = {
    "ç›¸å½“å¤š": "ğŸŸ¢",
    "è¾ƒå¤š": "ğŸ”µ",
    "ä¸­ç­‰": "ğŸŸ¡",
    "è¾ƒå°‘": "ğŸŸ ",
    "ç›¸å½“å°‘": "ğŸ”´",
}

# æ¸¸æˆæ€»ä½“è´¨é‡è¯„ä¼° emojiï¼ˆä¸ç¡®ä¿¡åº¦/ä¿¡æ¯é‡ä½¿ç”¨ä¸åŒä½“ç³»ï¼Œä¾¿äºç›´è§‚åŒºåˆ†ï¼‰
QUALITY_EMOJI = {
    "ç›¸å½“å¥½": "ğŸ’",
    "è¾ƒå¥½": "âœ¨",
    "ä¸­ç­‰": "â–",
    "è¾ƒå·®": "ğŸ‘",
    "ç›¸å½“å·®": "ğŸ’€",
}

# ä¿¡æ¯æ¥æºæ ‡ç­¾
INFO_SOURCE_WEB = "ğŸ“¡è”ç½‘æ£€ç´¢"
INFO_SOURCE_LOCAL = "ğŸ“šè®­ç»ƒæ•°æ®ä¸Steamè¯„æµ‹"

# ä¿¡æ¯æºæ•…éšœæ ‡æ³¨ï¼ˆç”¨äºç¬”è®°ä¸­æ ‡è®°å“ªä¸ªä¿¡æ¯æºä¸å¯ç”¨ï¼Œå¯æœç´¢"ä¸å¯ç”¨"ç­›é€‰ï¼‰
WARN_STEAM_UNAVAIL = "[âš ï¸Steamå•†åº—ä¸å¯ç”¨]"
WARN_GOOGLE_UNAVAIL = "[âš ï¸Googleä¸å¯ç”¨]"
WARN_AITOOL_UNAVAIL = "[âš ï¸è”ç½‘å·¥å…·ä¸å¯ç”¨]"

# ä¿¡æ¯è¿‡å°‘æ ‡è®°å…³é”®è¯ï¼ˆç”¨äºè¯†åˆ«ä¿¡æ¯ä¸è¶³çš„æ ‡æ³¨æ€§ç¬”è®°ï¼‰
INSUFFICIENT_INFO_MARKER = "â›”ä¿¡æ¯è¿‡å°‘"


class SteamNotesManager:
    """Steam ç¬”è®°çš„æ ¸å¿ƒè¯»å†™é€»è¾‘"""

    def __init__(self, notes_dir: str, cloud_uploader: SteamCloudUploader = None,
                 uploaded_hashes: dict = None):
        self.notes_dir = notes_dir
        self.cloud_uploader = cloud_uploader
        self._dirty_apps = set()  # æœ‰æœ¬åœ°æ”¹åŠ¨ä½†å°šæœªä¸Šä¼ è‡³äº‘çš„ app_id é›†åˆ
        self._uploaded_hashes = uploaded_hashes or {}  # {app_id: md5} æŒä¹…åŒ–ä¸Šä¼ è®°å½•
        # æ‰«æç¼“å­˜ï¼š{app_id: {mtime, note_count, notes, ai_info}}
        self._scan_cache = {}
        # å¯åŠ¨æ—¶æ ¹æ®æŒä¹…åŒ–å“ˆå¸Œé‡å»º dirty çŠ¶æ€
        self._rebuild_dirty_from_hashes()

    @staticmethod
    def _gen_id():
        """ç”Ÿæˆ 8 ä½éšæœºåå…­è¿›åˆ¶ IDï¼Œä¸ Steam åŸç”Ÿæ ¼å¼ä¸€è‡´"""
        return ''.join(random.choices('0123456789abcdef', k=8))

    @staticmethod
    def _wrap_content(text: str) -> str:
        """å°†çº¯æ–‡æœ¬åŒ…è£¹ä¸º [p]...[/p] æ ¼å¼ï¼ˆå¦‚æœå°šæœªåŒ…è£¹ï¼‰"""
        stripped = text.strip()
        # å¦‚æœå·²ç»åŒ…å«å¯Œæ–‡æœ¬æ ‡ç­¾ï¼Œä¸åšå¤„ç†
        if stripped.startswith('[p]') or stripped.startswith('[h1]') or \
           stripped.startswith('[h2]') or stripped.startswith('[h3]') or \
           stripped.startswith('[list]') or stripped.startswith('[olist]'):
            return stripped
        # æŒ‰æ®µè½åˆ†å‰²å¹¶åŒ…è£¹
        paragraphs = stripped.split('\n\n')
        wrapped = []
        for p in paragraphs:
            p = p.strip()
            if p:
                wrapped.append(f'[p]{p}[/p]')
        return ''.join(wrapped) if wrapped else f'[p]{stripped}[/p]'

    def _build_entry(self, app_id: str, title: str, content: str) -> dict:
        """æ„å»ºä¸€æ¡ç¬¦åˆ Steam åŸç”Ÿæ ¼å¼çš„ç¬”è®°æ¡ç›®"""
        now = int(time.time())
        return {
            "id": self._gen_id(),
            "appid": int(app_id) if app_id.isdigit() else app_id,
            "ordinal": 0,
            "time_created": now,
            "time_modified": now,
            "title": title,
            "content": self._wrap_content(content),
        }

    def _get_note_file(self, app_id: str) -> str:
        return os.path.join(self.notes_dir, f"notes_{app_id}")

    def read_notes(self, app_id: str) -> dict:
        """è¯»å–æŒ‡å®šæ¸¸æˆçš„ç¬”è®°æ–‡ä»¶"""
        path = self._get_note_file(app_id)
        if os.path.exists(path):
            try:
                with open(path, "r", encoding="utf-8") as f:
                    return json.load(f)
            except (json.JSONDecodeError, IOError, OSError) as e:
                # æ–‡ä»¶æŸåï¼šå¤‡ä»½åè¿”å›ç©ºï¼Œé˜²æ­¢åç»­å†™å…¥è¦†ç›–å¯¼è‡´æ°¸ä¹…ä¸¢å¤±
                try:
                    import shutil
                    shutil.copy2(path, path + ".corrupt")
                except Exception:
                    pass
                print(f"[ç¬”è®°] âš ï¸ notes_{app_id} è§£æå¤±è´¥å·²å¤‡ä»½: {e}")
        return {"notes": []}

    def read_notes_cached(self, app_id: str) -> dict:
        """ä»æ‰«æç¼“å­˜è¯»å–ç¬”è®°æ•°æ®ï¼Œç¼“å­˜æœªå‘½ä¸­æ—¶å›é€€åˆ°ç£ç›˜è¯»å–"""
        cached = self._scan_cache.get(app_id)
        if cached and 'notes' in cached:
            return {"notes": cached['notes']}
        return self.read_notes(app_id)

    def _scan_single_file(self, app_id: str, filepath: str) -> dict:
        """æ‰«æå•ä¸ªç¬”è®°æ–‡ä»¶ï¼Œè¿”å›ç¼“å­˜æ¡ç›®ï¼ˆå«ç¬”è®°æ•°æ® + AI ä¿¡æ¯ï¼‰"""
        try:
            mtime = os.path.getmtime(filepath)
        except OSError:
            return None
        cached = self._scan_cache.get(app_id)
        if cached and cached.get('mtime') == mtime:
            return cached
        try:
            with open(filepath, "r", encoding="utf-8") as fh:
                data = json.load(fh)
        except (json.JSONDecodeError, IOError, OSError):
            return None
        notes = data.get("notes", [])
        # æå– AI ä¿¡æ¯ï¼ˆä¸ scan_ai_notes ç›¸åŒé€»è¾‘ï¼‰
        ai_info = self._extract_ai_info(notes)
        entry = {
            'mtime': mtime,
            'note_count': len(notes),
            'file_path': filepath,
            'notes': notes,
            'ai_info': ai_info,
        }
        self._scan_cache[app_id] = entry
        return entry

    @staticmethod
    def _extract_ai_info(notes: list) -> dict | None:
        """ä»ç¬”è®°åˆ—è¡¨ä¸­æå– AI å…ƒæ•°æ®ï¼ˆæ¨¡å‹ã€ç¡®ä¿¡åº¦ã€ä¿¡æ¯é‡ç­‰ï¼‰"""
        models, indices, confidences = [], [], []
        info_volumes, info_sources, qualities = [], [], []
        has_insufficient = False
        for i, note in enumerate(notes):
            if not is_ai_note(note):
                continue
            model = extract_ai_model_from_note(note)
            if model and model not in models:
                models.append(model)
            conf = extract_ai_confidence_from_note(note)
            if conf and conf not in confidences:
                confidences.append(conf)
            vol = extract_ai_info_volume_from_note(note)
            if vol and vol not in info_volumes:
                info_volumes.append(vol)
            src = extract_ai_info_source_from_note(note)
            if src and src not in info_sources:
                info_sources.append(src)
            qual = extract_ai_quality_from_note(note)
            if qual and qual not in qualities:
                qualities.append(qual)
            if is_insufficient_info_note(note):
                has_insufficient = True
            indices.append(i)
        if not indices:
            return None
        return {
            'models': models, 'note_indices': indices,
            'note_count': len(indices), 'confidences': confidences,
            'info_volumes': info_volumes, 'info_sources': info_sources,
            'qualities': qualities, 'has_insufficient': has_insufficient,
        }

    def scan_all(self) -> tuple:
        """å•æ¬¡éå†ç›®å½•ï¼Œè¿”å› (notes_games, ai_notes_map)ã€‚
        ä½¿ç”¨ mtime ç¼“å­˜è·³è¿‡æœªå˜åŒ–çš„æ–‡ä»¶ã€‚
        """
        notes_games = {}
        ai_notes_map = {}
        if not os.path.exists(self.notes_dir):
            return notes_games, ai_notes_map
        current_aids = set()
        for f in os.listdir(self.notes_dir):
            if not f.startswith("notes_"):
                continue
            fp = os.path.join(self.notes_dir, f)
            if not os.path.isfile(fp):
                continue
            app_id = f[6:]  # strip "notes_" prefix
            if "::" in app_id:
                continue  # è·³è¿‡è„æ–‡ä»¶ï¼ˆå¦‚ notes_525480::lazyï¼‰
            current_aids.add(app_id)
            entry = self._scan_single_file(app_id, fp)
            if entry is None:
                continue
            notes_games[app_id] = {
                'app_id': app_id,
                'note_count': entry['note_count'],
                'file_path': entry['file_path'],
            }
            if entry['ai_info']:
                ai_notes_map[app_id] = entry['ai_info']
        # æ¸…ç†å·²åˆ é™¤æ–‡ä»¶çš„ç¼“å­˜
        for stale in set(self._scan_cache) - current_aids:
            del self._scan_cache[stale]
        return notes_games, ai_notes_map

    def invalidate_scan_cache(self, app_id: str = None):
        """æ‰‹åŠ¨å¤±æ•ˆæ‰«æç¼“å­˜ã€‚app_id=None æ—¶æ¸…ç©ºå…¨éƒ¨ã€‚"""
        if app_id:
            self._scan_cache.pop(app_id, None)
        else:
            self._scan_cache.clear()

    def write_notes(self, app_id: str, data: dict):
        """å†™å…¥ç¬”è®°æ–‡ä»¶ï¼ˆåŸå­å†™å…¥ï¼‰ï¼Œå¹¶æ ‡è®°ä¸ºéœ€è¦ä¸Šä¼ åˆ°äº‘"""
        os.makedirs(self.notes_dir, exist_ok=True)
        path = self._get_note_file(app_id)
        content = json.dumps(data, ensure_ascii=False, indent=2)
        tmp = path + ".tmp"
        with open(tmp, "w", encoding="utf-8") as f:
            f.write(content)
            f.flush()
            os.fsync(f.fileno())
        os.replace(tmp, path)
        self._dirty_apps.add(app_id)
        self._scan_cache.pop(app_id, None)

    def cloud_upload(self, app_id: str) -> bool:
        """ä¸Šä¼ æŒ‡å®š app çš„ç¬”è®°åˆ° Steam Cloudï¼ŒæˆåŠŸåæ¸…é™¤ dirty æ ‡è®°å¹¶è®°å½•å“ˆå¸Œ"""
        if not self.cloud_uploader or not self.cloud_uploader.initialized:
            return False
        path = self._get_note_file(app_id)
        if not os.path.exists(path):
            return False
        with open(path, "r", encoding="utf-8") as f:
            content = f.read()
        # ä¸Šä¼ å‰è®°å½•å†…å®¹å“ˆå¸Œï¼ˆåŸºäºå®é™…ä¸Šä¼ çš„å†…å®¹ï¼Œéç£ç›˜æ–‡ä»¶ï¼‰
        uploaded_hash = hashlib.md5(content.encode("utf-8")).hexdigest()
        filename = f"notes_{app_id}"
        if self.cloud_uploader.file_write(filename, content.encode("utf-8")):
            # ä¸Šä¼ åå¯¹æ¯”ï¼šå¦‚æœæ–‡ä»¶åœ¨ä¸Šä¼ æœŸé—´è¢«ä¿®æ”¹ï¼Œä¸æ¸…é™¤ dirty
            current_hash = self._compute_file_hash(path)
            if current_hash == uploaded_hash:
                self._dirty_apps.discard(app_id)
            self._uploaded_hashes[app_id] = uploaded_hash
            return True
        return False

    def cloud_upload_all_dirty(self) -> tuple:
        """ä¸Šä¼ æ‰€æœ‰æœ‰æ”¹åŠ¨çš„ç¬”è®°åˆ°äº‘ï¼Œè¿”å› (æˆåŠŸæ•°, å¤±è´¥æ•°)"""
        ok = fail = 0
        for app_id in list(self._dirty_apps):
            if self.cloud_upload(app_id):
                ok += 1
            else:
                fail += 1
        return ok, fail

    def cloud_upload_all_batch(self, progress_callback=None) -> tuple:
        """æ‰¹é‡ä¸Šä¼ æ‰€æœ‰ dirty ç¬”è®°ï¼ˆä½¿ç”¨ batch_file_write æ¶ˆé™¤é€æ¡å¾€è¿”å¼€é”€ï¼‰ã€‚

        Args:
            progress_callback: å¯é€‰å›è°ƒ (current, total, ok, fail)
        Returns: (ok_count, fail_count)
        """
        if not self.cloud_uploader or not self.cloud_uploader.initialized:
            return 0, 0
        dirty_ids = list(self._dirty_apps)
        if not dirty_ids:
            return 0, 0
        # å‡†å¤‡æ–‡ä»¶åˆ—è¡¨ï¼ŒåŒæ—¶è®°å½•ä¸Šä¼ å†…å®¹çš„å“ˆå¸Œï¼ˆç”¨äºä¸Šä¼ åå¯¹æ¯”ï¼‰
        file_list = []
        id_list = []  # [(app_id, path, uploaded_hash), ...]
        for app_id in dirty_ids:
            path = self._get_note_file(app_id)
            if not os.path.exists(path):
                continue
            with open(path, "r", encoding="utf-8") as f:
                content = f.read()
            content_bytes = content.encode("utf-8")
            uploaded_hash = hashlib.md5(content_bytes).hexdigest()
            file_list.append((f"notes_{app_id}", content_bytes))
            id_list.append((app_id, path, uploaded_hash))
        if not file_list:
            return 0, 0
        ok, fail = self.cloud_uploader.batch_file_write(
            file_list, progress_callback=progress_callback)
        # æ›´æ–° dirty çŠ¶æ€ï¼šbatch ä¸æŠ¥å‘Šé€æ¡æˆè´¥ï¼Œ
        # ä»…å½“å…¨éƒ¨æˆåŠŸæ—¶æ‰æ ‡è®°æ‰€æœ‰ä¸ºå·²åŒæ­¥ï¼›æœ‰å¤±è´¥åˆ™ä¿å®ˆä¿ç•™ dirty
        if fail == 0:
            for app_id, path, uploaded_hash in id_list:
                # å¯¹æ¯”ï¼šå¦‚æœæ–‡ä»¶åœ¨ä¸Šä¼ æœŸé—´è¢«ä¿®æ”¹ï¼Œä¸æ¸…é™¤ dirty
                current_hash = self._compute_file_hash(path)
                if current_hash == uploaded_hash:
                    self._dirty_apps.discard(app_id)
                self._uploaded_hashes[app_id] = uploaded_hash
        return ok, fail

    def is_dirty(self, app_id: str) -> bool:
        return app_id in self._dirty_apps

    def dirty_count(self) -> int:
        return len(self._dirty_apps)

    @staticmethod
    def _compute_file_hash(filepath: str) -> str:
        """è®¡ç®—æ–‡ä»¶å†…å®¹çš„ MD5 å“ˆå¸Œï¼ˆtext æ¨¡å¼è¯»å– + UTF-8 ç¼–ç ï¼Œä¸ä¸Šä¼ è·¯å¾„ä¸€è‡´ï¼‰"""
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                return hashlib.md5(f.read().encode("utf-8")).hexdigest()
        except Exception:
            return ""

    def _rebuild_dirty_from_hashes(self):
        """æ ¹æ®æŒä¹…åŒ–çš„ä¸Šä¼ å“ˆå¸Œä¸æœ¬åœ°æ–‡ä»¶å¯¹æ¯”ï¼Œé‡å»º dirty çŠ¶æ€"""
        if not os.path.exists(self.notes_dir):
            return
        for f in os.listdir(self.notes_dir):
            fp = os.path.join(self.notes_dir, f)
            if not os.path.isfile(fp) or not f.startswith("notes_"):
                continue
            app_id = f[6:]  # strip "notes_"
            if "::" in app_id:
                continue
            local_hash = self._compute_file_hash(fp)
            stored_hash = self._uploaded_hashes.get(app_id, "")
            if not stored_hash or local_hash != stored_hash:
                self._dirty_apps.add(app_id)

    def mark_as_synced(self, app_id: str) -> bool:
        """æ‰‹åŠ¨å°†æŒ‡å®š app æ ‡è®°ä¸ºå·²åŒæ­¥ï¼ˆè®°å½•å½“å‰æ–‡ä»¶å“ˆå¸Œï¼Œæ¸…é™¤ dirty çŠ¶æ€ï¼‰"""
        path = self._get_note_file(app_id)
        if not os.path.exists(path):
            return False
        self._uploaded_hashes[app_id] = self._compute_file_hash(path)
        self._dirty_apps.discard(app_id)
        return True

    def get_uploaded_hashes(self) -> dict:
        """è¿”å›å½“å‰ä¸Šä¼ å“ˆå¸Œè¡¨ï¼ˆä¾›å¤–éƒ¨æŒä¹…åŒ–ï¼‰"""
        return dict(self._uploaded_hashes)

    def create_note(self, app_id: str, title: str, content: str) -> dict:
        """åˆ›å»ºä¸€æ¡ç¬”è®°ï¼ˆå§‹ç»ˆè¿½åŠ ï¼‰"""
        entry = self._build_entry(app_id, title, content)
        data = self.read_notes(app_id)
        data["notes"].append(entry)
        self.write_notes(app_id, data)
        return self.read_notes(app_id)

    def update_note(self, app_id: str, index: int, title: str, content: str):
        """æ›´æ–°æŒ‡å®šç´¢å¼•çš„ç¬”è®°"""
        data = self.read_notes(app_id)
        notes = data.get("notes", [])
        if 0 <= index < len(notes):
            notes[index]["title"] = title
            notes[index]["content"] = self._wrap_content(content)
            notes[index]["time_modified"] = int(time.time())
            self.write_notes(app_id, data)
            return True
        return False

    def delete_note(self, app_id: str, index: int) -> bool:
        """åˆ é™¤æŒ‡å®šç´¢å¼•çš„ç¬”è®°
        
        Returns: True if deleted, False if invalid index
        """
        data = self.read_notes(app_id)
        notes = data.get("notes", [])
        if 0 <= index < len(notes):
            notes.pop(index)
            self.write_notes(app_id, data)
            return True
        return False

    def delete_notes_by_ids(self, app_id: str, note_ids: list) -> int:
        """åˆ é™¤æŒ‡å®šæ¸¸æˆä¸­ç‰¹å®š ID çš„ç¬”è®°

        Args:
            app_id: æ¸¸æˆ AppID
            note_ids: è¦åˆ é™¤çš„ç¬”è®° ID åˆ—è¡¨

        Returns: å®é™…åˆ é™¤çš„æ•°é‡
        """
        data = self.read_notes(app_id)
        notes = data.get("notes", [])
        ids_set = set(note_ids)
        original_len = len(notes)
        data["notes"] = [n for n in notes if n.get("id", "") not in ids_set]
        deleted = original_len - len(data["notes"])
        if deleted > 0:
            if data["notes"]:
                self.write_notes(app_id, data)
            else:
                # æ‰€æœ‰ç¬”è®°éƒ½è¢«åˆ é™¤äº†ï¼Œç›´æ¥åˆ é™¤æ–‡ä»¶
                self.delete_all_notes(app_id)
        return deleted

    def delete_all_notes(self, app_id: str) -> bool:
        """åˆ é™¤æŒ‡å®šæ¸¸æˆçš„æ‰€æœ‰ç¬”è®°"""
        path = self._get_note_file(app_id)
        if os.path.exists(path):
            os.remove(path)
            self._dirty_apps.discard(app_id)
            self._scan_cache.pop(app_id, None)
            # åŒæ—¶ä» Steam Cloud åˆ é™¤
            if self.cloud_uploader and self.cloud_uploader.initialized:
                self.cloud_uploader.file_delete(f"notes_{app_id}")
            return True
        return False

    def move_note(self, app_id: str, index: int, direction: int) -> bool:
        """ç§»åŠ¨ç¬”è®°é¡ºåºã€‚direction: -1=ä¸Šç§», +1=ä¸‹ç§»
        
        Returns: True if moved, False if invalid move
        """
        data = self.read_notes(app_id)
        notes = data.get("notes", [])
        new_index = index + direction
        
        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
        if not (0 <= index < len(notes) and 0 <= new_index < len(notes)):
            return False
        
        # æ‰§è¡Œç§»åŠ¨
        notes[index], notes[new_index] = notes[new_index], notes[index]
        self.write_notes(app_id, data)
        return True

    def list_all_games(self) -> list:
        """åˆ—å‡ºæ‰€æœ‰æœ‰ç¬”è®°çš„æ¸¸æˆ [{app_id, note_count, file_path}]"""
        notes_games, _ = self.scan_all()
        return sorted(notes_games.values(), key=lambda g: g['app_id'])

    # â”€â”€ æ‰¹é‡å¯¼å‡ºæ ¼å¼æ ‡è®° â”€â”€
    BATCH_EXPORT_HEADER = "# Steam Notes Batch Export"
    BATCH_APP_HEADER = "===APP_ID:"
    BATCH_NOTE_SEP = "---===NOTE_SEPARATOR===---"

    def export_single_note(self, app_id: str, note_index: int, output_path: str):
        """å¯¼å‡ºå•æ¡ç¬”è®°ä¸ºç‹¬ç«‹æ–‡ä»¶ï¼Œå†…å®¹ä¸º BBCode æºç """
        data = self.read_notes(app_id)
        notes = data.get("notes", [])
        if 0 <= note_index < len(notes):
            note = notes[note_index]
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(note.get("content", ""))

    def export_batch(self, app_ids: list, output_path: str, note_filter=None):
        """æ‰¹é‡å¯¼å‡ºå¤šä¸ªæ¸¸æˆçš„ç¬”è®°ä¸ºä¸€ä¸ªç»“æ„åŒ–æ–‡ä»¶
        note_filter: å¯é€‰çš„è¿‡æ»¤å‡½æ•°ï¼Œæ¥å— note dictï¼Œè¿”å› True è¡¨ç¤ºå¯¼å‡º
        """
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(f"{self.BATCH_EXPORT_HEADER}\n")
            f.write(f"# å¯¼å‡ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# åŒ…å«æ¸¸æˆæ•°: {len(app_ids)}\n\n")

            for app_id in app_ids:
                data = self.read_notes(app_id)
                notes = data.get("notes", [])
                if note_filter:
                    notes = [n for n in notes if note_filter(n)]
                if not notes:
                    continue
                f.write(f"{self.BATCH_APP_HEADER}{app_id}===\n")
                f.write(f"# ç¬”è®°æ•°é‡: {len(notes)}\n\n")
                for i, note in enumerate(notes):
                    if i > 0:
                        f.write(f"\n{self.BATCH_NOTE_SEP}\n\n")
                    f.write(f"## {note.get('title', '(æ— æ ‡é¢˜)')}\n\n")
                    f.write(note.get("content", "") + "\n")
                f.write("\n")

    def import_single_note(self, app_id: str, title: str, file_path: str) -> dict:
        """ä»æ–‡ä»¶å¯¼å…¥å•æ¡ç¬”è®°ï¼ˆå§‹ç»ˆè¿½åŠ ï¼‰"""
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
        entry = self._build_entry(app_id, title, content)
        data = self.read_notes(app_id)
        data["notes"].append(entry)
        self.write_notes(app_id, data)
        # é‡æ–°è¯»å–ä»¥è¿”å›æœ€æ–°æ•°æ®
        return self.read_notes(app_id)

    @staticmethod
    def parse_batch_file(file_path: str) -> dict:
        """è§£ææ‰¹é‡å¯¼å‡ºæ–‡ä»¶ä½†ä¸å†™å…¥ã€‚
        Returns: {app_id: [entry_dict, ...], ...}
        æ¯ä¸ª entry_dict åŒ…å« title, content (åŸå§‹æ–‡æœ¬ï¼Œå°šæœª build_entry)
        """
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()

        result = {}
        app_sections = re.split(r'===APP_ID:(\S+?)===', content)
        i = 1
        while i < len(app_sections) - 1:
            app_id = app_sections[i].strip()
            section = app_sections[i + 1]
            i += 2

            note_blocks = section.split(SteamNotesManager.BATCH_NOTE_SEP)
            entries = []
            for block in note_blocks:
                lines = block.strip().split('\n')
                title = None
                content_lines = []
                for line in lines:
                    if line.startswith('# '):
                        continue
                    if line.startswith('## ') and title is None:
                        title = line[3:].strip()
                        continue
                    content_lines.append(line)
                body = '\n'.join(content_lines).strip()
                if title and body:
                    entries.append({"title": title, "content": body})
                elif title:
                    entries.append({"title": title, "content": ""})

            if entries:
                result[app_id] = entries

        return result

    def apply_batch_import(self, parsed: dict, ai_policy: str = "append",
                           per_app_policy: dict = None) -> dict:
        """å°†è§£æåçš„æ•°æ®å†™å…¥ç¬”è®°æ–‡ä»¶ã€‚
        parsed: {app_id: [{title, content}, ...]}
        ai_policy: å…¨å±€ AI å†²çªç­–ç•¥
            "append"  â€” AI ç¬”è®°è¿½åŠ åœ¨å·²æœ‰ç¬”è®°ä¹‹å
            "replace" â€” åˆ é™¤å·²æœ‰ AI ç¬”è®°ï¼Œå†å†™å…¥æ–° AI ç¬”è®°
            "skip_ai" â€” è·³è¿‡å¯¼å…¥æ–‡ä»¶ä¸­çš„ AI ç¬”è®°ï¼ˆä»…å¯¼å…¥é AI ç¬”è®°ï¼‰
        per_app_policy: {app_id: "replace"/"append"/"skip"} é€ä¸€è¦†ç›–å…¨å±€ç­–ç•¥
        Returns: {app_id: imported_count, ...}
        """
        if per_app_policy is None:
            per_app_policy = {}
        results = {}

        for app_id, entries in parsed.items():
            policy = per_app_policy.get(app_id, ai_policy)
            data = self.read_notes(app_id)
            existing = data.get("notes", [])

            to_import = []
            for e in entries:
                note = self._build_entry(app_id, e["title"], e["content"])
                is_ai = is_ai_note(note)
                if is_ai and policy == "skip_ai":
                    continue
                to_import.append((note, is_ai))

            if policy == "replace":
                # ç§»é™¤å·²æœ‰ AI ç¬”è®°
                existing = [n for n in existing if not is_ai_note(n)]

            for note, _ in to_import:
                existing.append(note)

            data["notes"] = existing
            self.write_notes(app_id, data)
            imported = len(to_import)
            if imported > 0:
                results[app_id] = imported

        return results

    def export_individual_files(self, app_ids: list, output_dir: str,
                               note_filter=None) -> tuple:
        """é€æ¡å¯¼å‡ºï¼šæ¯æ¡ç¬”è®°å¯¼å‡ºä¸ºç‹¬ç«‹ txt æ–‡ä»¶ï¼ˆæ–‡ä»¶å=ç¬”è®°æ ‡é¢˜ï¼Œå†…å®¹=BBCode æºç ï¼‰

        note_filter: å¯é€‰çš„è¿‡æ»¤å‡½æ•°ï¼Œæ¥å— note dictï¼Œè¿”å› True è¡¨ç¤ºå¯¼å‡º
        ä¸ºé¿å…æ–‡ä»¶åå†²çªï¼ŒåŒåç¬”è®°è‡ªåŠ¨è¿½åŠ åºå·åç¼€ã€‚
        Returns: (total_files: int, total_notes: int)
        """
        os.makedirs(output_dir, exist_ok=True)
        used_names = {}  # {safe_name: count} ç”¨äºå»é‡
        total_files = 0
        total_notes = 0
        for app_id in app_ids:
            data = self.read_notes(app_id)
            notes = data.get("notes", [])
            if note_filter:
                notes = [n for n in notes if note_filter(n)]
            for note in notes:
                total_notes += 1
                title = note.get("title", "untitled")
                content = note.get("content", title)
                safe_name = sanitize_filename(title)
                # å»é‡ï¼šå¦‚æœåŒååˆ™è¿½åŠ åºå·
                if safe_name in used_names:
                    used_names[safe_name] += 1
                    final_name = f"{safe_name}_{used_names[safe_name]}"
                else:
                    used_names[safe_name] = 0
                    final_name = safe_name
                filepath = os.path.join(output_dir, f"{final_name}.txt")
                with open(filepath, "w", encoding="utf-8") as f:
                    f.write(content)
                total_files += 1
        return total_files, total_notes

    def scan_ai_notes(self) -> dict:
        """æ‰«ææ‰€æœ‰ç¬”è®°ï¼Œè¯†åˆ« AI å¤„ç†è¿‡çš„æ¸¸æˆï¼ˆå§”æ‰˜ç»™ scan_allï¼‰"""
        _, ai_notes_map = self.scan_all()
        return ai_notes_map

    def backfill_ai_note_dates(self) -> tuple:
        """ä¸ºæ‰€æœ‰ç¼ºå°‘ç”Ÿæˆæ—¥æœŸçš„ AI ç¬”è®°è¡¥ä¸Šæ—¥æœŸï¼ˆä½¿ç”¨ time_createdï¼‰ã€‚

        Returns: (updated_apps: int, updated_notes: int)
        """
        import re as _re
        date_pattern = _re.compile(r'ğŸ“…ç”Ÿæˆäº \d{4}-\d{2}-\d{2}')
        updated_apps = 0
        updated_notes = 0
        if not os.path.exists(self.notes_dir):
            return 0, 0
        for f in os.listdir(self.notes_dir):
            fp = os.path.join(self.notes_dir, f)
            if not os.path.isfile(fp) or not f.startswith("notes_"):
                continue
            app_id = f[6:]
            if "::" in app_id:
                continue
            try:
                with open(fp, "r", encoding="utf-8") as fh:
                    data = json.load(fh)
            except Exception:
                continue
            notes = data.get("notes", [])
            changed = False
            for note in notes:
                if not is_ai_note(note):
                    continue
                title = note.get("title", "")
                if date_pattern.search(title):
                    continue  # å·²æœ‰æ—¥æœŸ
                ts = note.get("time_created", note.get("time_modified", 0))
                if ts:
                    date_str = datetime.fromtimestamp(ts).strftime("%Y-%m-%d")
                else:
                    date_str = "æœªçŸ¥æ—¥æœŸ"
                suffix = f" ğŸ“…ç”Ÿæˆäº {date_str}"
                note["title"] = title + suffix
                # æ›´æ–° contentï¼šåœ¨æœ€åä¸€ä¸ª [/p] å‰æ’å…¥æ—¥æœŸ
                content = note.get("content", "")
                if content.rstrip().endswith("[/p]"):
                    note["content"] = content.rstrip()[:-4] + suffix + "[/p]"
                else:
                    note["content"] = content + suffix
                changed = True
                updated_notes += 1
            if changed:
                self.write_notes(app_id, data)
                updated_apps += 1
        return updated_apps, updated_notes

    def find_duplicate_notes(self) -> list:
        """æ‰«ææ‰€æœ‰ç¬”è®°ï¼Œæ‰¾åˆ°æ ‡é¢˜+å†…å®¹å®Œå…¨ç›¸åŒçš„é‡å¤é¡¹ã€‚

        Returns: [{app_id, title, content, indices: [int], count: int}, ...]
        æ¯ä¸ªæ¡ç›®ä»£è¡¨ä¸€ç»„é‡å¤ç¬”è®°ï¼ˆåŒä¸€æ¸¸æˆå†…ï¼‰ï¼Œindices ä¸ºè¯¥ç»„æ‰€æœ‰å‰¯æœ¬çš„ç´¢å¼•ã€‚
        """
        duplicates = []
        if not os.path.exists(self.notes_dir):
            return duplicates
        for f in os.listdir(self.notes_dir):
            fp = os.path.join(self.notes_dir, f)
            if not os.path.isfile(fp) or not f.startswith("notes_"):
                continue
            app_id = f.replace("notes_", "")
            try:
                with open(fp, "r", encoding="utf-8") as fh:
                    data = json.load(fh)
                notes = data.get("notes", [])
                # æŒ‰ (title, content) åˆ†ç»„
                seen = {}  # {(title, content): [index, ...]}
                for i, note in enumerate(notes):
                    key = (note.get("title", ""), note.get("content", ""))
                    if key not in seen:
                        seen[key] = []
                    seen[key].append(i)
                for (title, content), indices in seen.items():
                    if len(indices) > 1:
                        duplicates.append({
                            'app_id': app_id,
                            'title': title,
                            'content': content,
                            'indices': indices,
                            'count': len(indices),
                        })
            except Exception:
                continue
        return duplicates

    def delete_duplicate_notes(self, app_id: str, indices_to_remove: list) -> int:
        """åˆ é™¤æŒ‡å®šæ¸¸æˆä¸­çš„é‡å¤ç¬”è®°ï¼ˆæŒ‰ç´¢å¼•åˆ—è¡¨ï¼Œä»å¤§åˆ°å°åˆ é™¤é¿å…ç´¢å¼•åç§»ï¼‰

        Returns: å®é™…åˆ é™¤çš„æ•°é‡
        """
        data = self.read_notes(app_id)
        notes = data.get("notes", [])
        removed = 0
        for idx in sorted(indices_to_remove, reverse=True):
            if 0 <= idx < len(notes):
                notes.pop(idx)
                removed += 1
        if removed > 0:
            data["notes"] = notes
            if notes:
                self.write_notes(app_id, data)
            else:
                # æ²¡æœ‰ç¬”è®°äº†ï¼Œåˆ é™¤æ–‡ä»¶
                path = self._get_note_file(app_id)
                if os.path.exists(path):
                    os.remove(path)
                self._dirty_apps.discard(app_id)
                self._scan_cache.pop(app_id, None)
                if self.cloud_uploader and self.cloud_uploader.initialized:
                    self.cloud_uploader.file_delete(f"notes_{app_id}")
        return removed
