"""
core_scraper.py â€” Steam é¡µé¢æŠ“å– Mixin

åŒ…å«ï¼šJSON è¯»å†™ã€æ”¶è—å¤¹è§£æã€HTML AppID æå–ã€é‰´èµå®¶ API æŠ“å–ã€
é€šç”¨åˆ—è¡¨æŠ“å–ã€SteamDB è§£æã€å¢é‡/æ›¿æ¢æ›´æ–°ã€å¯¼å…¥/å¯¼å‡ºã€Steam250ã€‚
"""

import json
import os
import re
import secrets
import time

from utils import steam_sort_key
import urllib.error
import urllib.parse
import urllib.request
from datetime import datetime
from json import JSONDecodeError


class ScraperMixin:
    """Steam é¡µé¢æŠ“å–ä¸æ”¶è—å¤¹æ“ä½œï¼ˆMixinï¼Œself æŒ‡å‘ SteamToolboxCore å®ä¾‹ï¼‰"""

    def load_json(self):
        if not self.current_account.storage_path or not os.path.exists(self.current_account.storage_path):
            print("[CollectionsCore] é”™è¯¯: è¯»å–æ–‡ä»¶å¤±è´¥ï¼Œè¯·ç¡®ä¿å·²é€‰æ‹©æœ‰æ•ˆçš„ Steam è´¦å·ã€‚")
            return None
        try:
            with open(self.current_account.storage_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"[CollectionsCore] è¯»å–é”™è¯¯: è§£æå¤±è´¥: {e}")
            return None

    def save_json(self, data, create_backup=True, backup_description=""):
        """ä¿å­˜ JSON æ•°æ®åˆ°åŸæ–‡ä»¶

        Args:
            data: è¦ä¿å­˜çš„æ•°æ®
            create_backup: æ˜¯å¦åœ¨ä¿å­˜å‰åˆ›å»ºå¤‡ä»½
            backup_description: å¤‡ä»½æè¿°

        Returns:
            (bool, str): (æ˜¯å¦æˆåŠŸ, ä¿¡æ¯æ–‡æœ¬)
        """
        if not self.current_account.storage_path:
            return False, "æœªé€‰æ‹©è´¦å·ï¼Œæ— æ³•ä¿å­˜ã€‚"

        # åˆ›å»ºå¤‡ä»½
        if create_backup and self.backup_manager:
            backup_path = self.backup_manager.create_backup(description=backup_description)
            if backup_path:
                backup_info = f"\nå·²è‡ªåŠ¨å¤‡ä»½è‡³: {os.path.basename(backup_path)}"
            else:
                backup_info = "\nâš ï¸ å¤‡ä»½åˆ›å»ºå¤±è´¥"
        else:
            backup_info = ""

        # å†™å…¥åŸæ–‡ä»¶ï¼ˆä½¿ç”¨åŸå­å†™å…¥ï¼‰
        tmp_path = self.current_account.storage_path + ".tmp"
        try:
            with open(tmp_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, separators=(',', ':'))

            if os.path.exists(self.current_account.storage_path):
                os.replace(tmp_path, self.current_account.storage_path)
            else:
                os.rename(tmp_path, self.current_account.storage_path)

            msg = f"æ–‡ä»¶å·²ä¿å­˜ï¼š{os.path.basename(self.current_account.storage_path)}{backup_info}"
            return True, msg
        except Exception as e:
            if os.path.exists(tmp_path):
                try:
                    os.remove(tmp_path)
                except Exception:
                    pass
            return False, f"æ— æ³•å†™å…¥æ–‡ä»¶: {e}"
            return False


    def get_static_collections(self, data):
        """è·å–æ‰€æœ‰æ”¶è—å¤¹ï¼ˆå«åŠ¨æ€ï¼‰åŠå…¶ entry å¼•ç”¨ï¼ŒæŒ‰å­—æ¯æ’åº"""
        return self.get_all_collections_with_refs(data)

    @staticmethod
    def get_all_collections_with_refs(data):
        """è·å–æ‰€æœ‰æ”¶è—å¤¹ï¼ˆå«åŠ¨æ€æ”¶è—å¤¹ï¼‰åŠå…¶ entry å¼•ç”¨ï¼ŒæŒ‰å­—æ¯æ’åº"""
        collections = []
        for entry in data:
            key = entry[0]
            meta = entry[1]
            if key.startswith("user-collections."):
                if meta.get("is_deleted") is True or "value" not in meta:
                    continue
                try:
                    val_obj = json.loads(meta['value'])
                    is_dynamic = "filterSpec" in val_obj
                    icon = "ğŸ”" if is_dynamic else "ğŸ“"
                    collections.append({
                        "entry_ref": entry,
                        "id": val_obj.get("id"),
                        "name": val_obj.get("name"),
                        "added": val_obj.get("added", []),
                        "is_dynamic": is_dynamic,
                        "display_name": f"{icon} {val_obj.get('name', 'æœªå‘½å')}"
                    })
                except Exception:
                    continue
        collections.sort(key=lambda c: steam_sort_key(c.get('name') or ''))
        return collections

    @staticmethod
    def get_all_collections_ordered(data):
        """è·å–æ‰€æœ‰æ”¶è—å¤¹ï¼ˆæŒ‰å­—æ¯é¡ºåºæ’åºï¼Œä¸ Steam å®¢æˆ·ç«¯ä¸€è‡´ï¼‰"""
        collections = []
        for entry in data:
            key = entry[0]
            meta = entry[1]
            if key.startswith("user-collections."):
                if meta.get("is_deleted") is True or "value" not in meta:
                    continue
                try:
                    val_obj = json.loads(meta['value'])
                    is_dynamic = "filterSpec" in val_obj
                    col_info = {
                        "id": val_obj.get("id"),
                        "name": val_obj.get("name", "æœªå‘½å"),
                        "added": val_obj.get("added", []),
                        "removed": val_obj.get("removed", []),
                        "is_dynamic": is_dynamic
                    }
                    if is_dynamic:
                        col_info["filterSpec"] = val_obj.get("filterSpec")
                    collections.append(col_info)
                except Exception:
                    continue
        collections.sort(key=lambda c: steam_sort_key(c['name']))
        return collections

    @staticmethod
    def extract_ids_from_html(html_text):
        """æ ¸å¿ƒæå–é€»è¾‘ï¼šä» HTML ä¸­æå– AppID"""
        search_area = html_text
        list_start = html_text.find('id="RecommendationsRows"')
        if list_start == -1:
            list_start = html_text.find('class="creator_grid_ctn"')

        if list_start != -1:
            footer_start = html_text.find('id="footer"', list_start)
            search_area = html_text[list_start: (footer_start if footer_start != -1 else len(html_text))]

        raw_matches = re.findall(r'data-ds-appid="([\d,]+)"', search_area)
        all_ids = []
        for m in raw_matches:
            if ',' in m:
                all_ids.extend(m.split(','))
            else:
                all_ids.append(m)

        # å¦‚æœ data-ds-appid æœªæ‰¾åˆ°ï¼Œå›é€€åˆ°ä» store.steampowered.com/app/ URL ä¸­æå–
        if not all_ids:
            app_url_matches = re.findall(r'store\.steampowered\.com/app/(\d+)', search_area)
            all_ids = app_url_matches

        return list(dict.fromkeys([int(aid) for aid in all_ids if aid.isdigit()]))

    def extract_page_name_from_html(self, html_text, url_hint=""):
        """ä» HTML ä¸­æ™ºèƒ½æå–é¡µé¢åç§°ï¼ˆå¸¦ç±»å‹å‰ç¼€ï¼‰"""
        type_name_cn = "åˆ—è¡¨"
        if url_hint:
            page_type, _ = self.extract_steam_list_info(url_hint)
            type_names = {
                "curator": "é‰´èµå®¶",
                "publisher": "å‘è¡Œå•†",
                "developer": "å¼€å‘å•†",
                "franchise": "ç³»åˆ—",
                "genre": "ç±»å‹",
                "category": "åˆ†ç±»",
            }
            type_name_cn = type_names.get(page_type, "åˆ—è¡¨")

        if "curator" in html_text.lower() or "é‰´èµå®¶" in html_text:
            type_name_cn = "é‰´èµå®¶"
        elif "publisher" in html_text.lower():
            type_name_cn = "å‘è¡Œå•†"
        elif "developer" in html_text.lower():
            type_name_cn = "å¼€å‘å•†"

        name = None
        match = re.search(r'class="curator_name".*?><a.*?>(.*?)</a>', html_text, re.S)
        if match:
            name = match.group(1).strip()

        if not name:
            match = re.search(r'<title>(.*?)</title>', html_text, re.I)
            if match:
                title = match.group(1)
                title = re.sub(r'\s*[-â€“â€”]\s*Steam.*$', '', title, flags=re.I)
                title = re.sub(r'\s*on Steam.*$', '', title, flags=re.I)
                title = re.sub(r'^Steam é‰´èµå®¶ï¼š', '', title)
                title = re.sub(r'^Steam Curator:\s*', '', title, flags=re.I)
                name = title.strip()

        if name:
            return f"{type_name_cn}ï¼š{name}"
        return f"{type_name_cn}ï¼šæœªçŸ¥"

    @staticmethod
    def extract_steam_list_info(url_or_id):
        """ä» URL æˆ–ç›´æ¥è¾“å…¥ä¸­æå– Steam åˆ—è¡¨é¡µé¢ä¿¡æ¯"""
        text = url_or_id.strip()

        if text.isdigit():
            return "curator", text

        patterns = [
            (r'/curator/(\d+)', "curator"),
            (r'/publisher/([^/?#]+)', "publisher"),
            (r'/developer/([^/?#]+)', "developer"),
            (r'/franchise/([^/?#]+)', "franchise"),
            (r'/genre/([^/?#]+)', "genre"),
            (r'/category/([^/?#]+)', "category"),
        ]

        for pattern, page_type in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return page_type, match.group(1)

        return None, None

    def fetch_steam_list(self, page_type, identifier, progress_callback=None, login_cookies=None):
        """é€šè¿‡ Steam API è‡ªåŠ¨è·å–åˆ—è¡¨é¡µé¢çš„æ‰€æœ‰æ¸¸æˆ"""
        type_names = {
            "curator": "é‰´èµå®¶",
            "publisher": "å‘è¡Œå•†",
            "developer": "å¼€å‘å•†",
            "franchise": "ç³»åˆ—",
            "genre": "ç±»å‹",
            "category": "åˆ†ç±»",
        }
        type_name_cn = type_names.get(page_type, "åˆ—è¡¨")

        has_login = login_cookies is not None and len(login_cookies.strip()) > 0

        if has_login:
            cookies = f"{login_cookies}; {self._BASE_COOKIES}"
        else:
            cookies = self._BASE_COOKIES

        if page_type in ("curator", "publisher", "developer"):
            return self.fetch_curator_style_api(page_type, identifier, type_name_cn, cookies, has_login,
                                                 progress_callback)
        else:
            return self.fetch_generic_list(page_type, identifier, type_name_cn, cookies, has_login, progress_callback)

    _BASE_COOKIES = "birthtime=283993201; wants_mature_content=1; mature_content=1; lastagecheckage=1-0-1979; steamCountry=US%7C0"

    def fetch_curator_style_api(self, page_type, identifier, type_name_cn, cookies, has_login, progress_callback=None):
        """ç»Ÿä¸€çš„ ajaxgetfilteredrecommendations API æŠ“å–"""
        from urllib.parse import unquote

        page_url = f"https://store.steampowered.com/{page_type}/{identifier}/"
        # HTML é¡µé¢è·å–åªç”¨åŸºç¡€ cookieï¼ˆå¹´é¾„éªŒè¯ï¼‰ï¼Œä¸å¸¦ç™»å½• cookie
        # Steam å¯¹ developer/publisher é¡µé¢åœ¨å¸¦ steamLoginSecure æ—¶ä¼šäº§ç”Ÿé‡å®šå‘å¾ªç¯
        headers_html = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Cookie': self._BASE_COOKIES,
        }

        # é˜¶æ®µ1ï¼šè§£æ curator_id å’Œé¡µé¢åç§°
        resolved = self._resolve_curator_info(
            page_type, identifier, page_url, headers_html, progress_callback)
        if resolved is None:
            return self.fetch_generic_list(
                page_type, identifier, type_name_cn, cookies, has_login, progress_callback)
        curator_id, page_name = resolved

        # é˜¶æ®µ2ï¼šå¤šè¯­è¨€åˆ†é¡µ API æŠ“å–
        all_unique_ids, page_name = self._paginate_curator_api(
            curator_id, page_url, cookies, page_name, progress_callback)

        # é˜¶æ®µ3ï¼šç»„è£…ç»“æœ
        if not all_unique_ids:
            return [], None, f"è¯¥{type_name_cn}æ²¡æœ‰ä»»ä½•æ¸¸æˆï¼Œæˆ–æ ‡è¯†ç¬¦æ— æ•ˆã€‚\nè¯·æ£€æŸ¥ URL æ˜¯å¦æ­£ç¡®ã€‚", has_login

        if page_name:
            display_name = f"{type_name_cn}ï¼š{page_name}"
        else:
            display_name = f"{type_name_cn}ï¼š{unquote(identifier)}"
        return list(all_unique_ids), display_name, None, has_login

    def _resolve_curator_info(self, page_type, identifier, page_url, headers_html, progress_callback):
        """è§£æé‰´èµå®¶/å‘è¡Œå•†/å¼€å‘å•†é¡µé¢ï¼Œæå– curator_id å’Œé¡µé¢åç§°ã€‚

        Returns:
            (curator_id, page_name) æˆ– Noneï¼ˆè¡¨ç¤ºéœ€è¦å›é€€åˆ°é€šç”¨æŠ“å–ï¼‰
        """
        curator_id = None
        page_name = None

        if page_type == "curator":
            curator_id = identifier
            if progress_callback:
                progress_callback(0, 0, "æ­£åœ¨éªŒè¯é‰´èµå®¶é¡µé¢...", "æ­£åœ¨è¿æ¥ Steam å•†åº—...")
            try:
                req = urllib.request.Request(page_url, headers=headers_html)
                with urllib.request.urlopen(req, timeout=30, context=self.ssl_context) as resp:
                    html_content = resp.read().decode('utf-8')
                page_name = self._extract_name_from_html(html_content, [
                    r'class="curator_name"[^>]*>.*?<a[^>]*>(.*?)</a>',
                    r'<title>Steam é‰´èµå®¶ï¼š([^<]+?)</title>',
                    r'<title>([^<]+?)(?:\s*[-â€“â€”]\s*Steam)?</title>',
                ])
            except Exception:
                pass
        else:
            if progress_callback:
                progress_callback(0, 0, "æ­£åœ¨è·å–é¡µé¢ä¿¡æ¯...", f"æ­£åœ¨è®¿é—® {page_type}/{identifier} ...")
            try:
                req = urllib.request.Request(page_url, headers=headers_html)
                with urllib.request.urlopen(req, timeout=30, context=self.ssl_context) as resp:
                    html_content = resp.read().decode('utf-8')
            except Exception:
                return None

            clanid_patterns = [
                r'curator_clanid[=:][\s"\']*(\d+)',
                r'IgnoreCurator\(\s*(\d+)',
                r'newshub/group/(\d+)',
                r'data-clanid=["\']?(\d+)',
                r'"clanAccountID"\s*:\s*(\d+)',
            ]
            for pattern in clanid_patterns:
                clanid_match = re.search(pattern, html_content)
                if clanid_match:
                    curator_id = clanid_match.group(1)
                    break

            page_name = self._extract_name_from_html(html_content, [
                r'class="curator_name"[^>]*>.*?<a[^>]*>(.*?)</a>',
                r'<title>(?:Steam (?:Publisher|Developer):\s*)?([^<]+?)(?:\s*[-â€“â€”]\s*Steam)?</title>',
            ])

        return (curator_id, page_name) if curator_id else None

    @staticmethod
    def _extract_name_from_html(html_content, patterns):
        """ä» HTML ä¸­æŒ‰ä¼˜å…ˆçº§åŒ¹é…åç§°ï¼Œè¿”å›ç¬¬ä¸€ä¸ªæœ‰æ•ˆåŒ¹é…æˆ– None"""
        for pattern in patterns:
            match = re.search(pattern, html_content, re.S | re.I)
            if match:
                extracted = re.sub(r'<[^>]+>', '', match.group(1)).strip()
                extracted = extracted.replace('&amp;', '&').replace('&quot;', '"')
                if extracted and len(extracted) < 100:
                    return extracted
        return None

    def _proxy_adult_scan(self, curator_id, cookies, progress_callback):
        """é€šè¿‡ç³»ç»Ÿä»£ç† + è´­ç‰©è½¦æ¢åŒºæŠ“å–æˆäººæ¸¸æˆã€‚

        Steam æŒ‰ IP è¿‡æ»¤æˆäººå†…å®¹ï¼Œä¸”åŒ¿åç”¨æˆ·å³ä½¿éå—é™ IP ä¹Ÿè¢«è¿‡æ»¤ã€‚
        éœ€è¦ï¼šç³»ç»Ÿä»£ç†ï¼ˆéå—é™å‡ºå£ IPï¼‰+ ç™»å½• cookie + setcountry æ¢åŒºã€‚

        Returns:
            set[int]: å‘ç°çš„ app ID é›†åˆï¼Œå¤±è´¥è¿”å›ç©º set
        """
        import http.cookiejar
        from http.cookiejar import Cookie

        proxies = urllib.request.getproxies()
        if not proxies.get('https') and not proxies.get('http'):
            return set()
        slc_match = re.search(r'steamLoginSecure=([^;]+)', cookies)
        if not slc_match:
            return set()

        if progress_callback:
            progress_callback(0, 0, "æ£€æµ‹åˆ°ç³»ç»Ÿä»£ç†", "ğŸ” æ­£åœ¨é€šè¿‡ä»£ç†æ‰«ææˆäººå†…å®¹...")

        jar = http.cookiejar.CookieJar()
        opener = urllib.request.build_opener(
            urllib.request.ProxyHandler(proxies),
            urllib.request.HTTPSHandler(context=self.ssl_context),
            urllib.request.HTTPCookieProcessor(jar),
        )
        # æ³¨å…¥ steamLoginSecure
        jar.set_cookie(Cookie(
            0, 'steamLoginSecure', slc_match.group(1), None, False,
            'store.steampowered.com', False, True, '/', True, True,
            int(time.time()) + 86400, False, None, None, {}))

        ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0'
        try:
            # å»ºç«‹ session
            opener.open(urllib.request.Request(
                'https://store.steampowered.com/', headers={'User-Agent': ua}
            ), timeout=15).read()
        except Exception:
            return set()

        sid = next((c.value for c in jar if c.name == 'sessionid'), None)
        if not sid:
            return set()

        orig_cc = next((c.value[:2] for c in jar if c.name == 'steamCountry'), 'CN')
        all_ids = set()
        scan_regions = ['US', 'JP']
        try:
            for cc in scan_regions:
                try:
                    body = urllib.parse.urlencode({'sessionid': sid, 'cc': cc}).encode()
                    opener.open(urllib.request.Request(
                        'https://store.steampowered.com/country/setcountry', data=body,
                        headers={'User-Agent': ua, 'Content-Type': 'application/x-www-form-urlencoded',
                                 'Referer': 'https://store.steampowered.com/cart/'},
                    ), timeout=15)
                except Exception:
                    continue
                region_ids = self._proxy_paginate(opener, curator_id, ua)
                new = region_ids - all_ids
                all_ids.update(region_ids)
                if progress_callback:
                    progress_callback(0, 0, "æ£€æµ‹åˆ°ç³»ç»Ÿä»£ç†",
                                      f"ğŸ” {cc} åŒºæ‰«æå®Œæˆï¼ˆ+{len(new)} æ–°ï¼‰")
                if not new and cc != scan_regions[0]:
                    break  # æ— æ–°å¢ï¼Œè·³è¿‡å‰©ä½™åŒºåŸŸ
        finally:
            try:
                body = urllib.parse.urlencode({'sessionid': sid, 'cc': orig_cc}).encode()
                opener.open(urllib.request.Request(
                    'https://store.steampowered.com/country/setcountry', data=body,
                    headers={'User-Agent': ua, 'Content-Type': 'application/x-www-form-urlencoded',
                             'Referer': 'https://store.steampowered.com/cart/'},
                ), timeout=15)
            except Exception:
                pass
        return all_ids

    def _proxy_paginate(self, opener, curator_id, ua):
        """é€šè¿‡ opener åˆ†é¡µæŠ“å– curator API çš„å…¨éƒ¨ appidã€‚"""
        base = f"https://store.steampowered.com/curator/{curator_id}/ajaxgetfilteredrecommendations/"
        all_ids = set()
        start, count = 0, 100
        total = None
        while True:
            req = urllib.request.Request(f"{base}?start={start}&count={count}", headers={
                'User-Agent': ua, 'Accept': 'application/json, */*',
                'X-Requested-With': 'XMLHttpRequest',
            })
            resp = opener.open(req, timeout=30)
            data = json.loads(resp.read().decode('utf-8'))
            if not data.get('success'):
                break
            if total is None:
                total = int(data.get('total_count', 0))
                if total == 0:
                    break
            html = data.get('results_html', '')
            ids = {int(x) for x in re.findall(r'data-ds-appid="(\d+)"', html)}
            new = ids - all_ids
            all_ids.update(ids)
            start += count
            if not new or start >= total:
                break
            time.sleep(0.05)
        return all_ids

    _LANG_CONFIGS = [
        ("schinese", "zh-CN,zh;q=0.9,en;q=0.8", "ç®€ä½“ä¸­æ–‡", "CN"),
        ("english", "en-US,en;q=0.9", "English", "US"),
        ("japanese", "ja,en;q=0.8", "æ—¥æœ¬èª", "JP"),
        ("tchinese", "zh-TW,zh;q=0.9,en;q=0.8", "ç¹é«”ä¸­æ–‡", "TW"),
        ("koreana", "ko,en;q=0.8", "í•œêµ­ì–´", "KR"),
    ]

    def _paginate_curator_api(self, curator_id, page_url, cookies, page_name, progress_callback):
        """å¤šè¯­è¨€å¹¶è¡Œåˆ†é¡µæŠ“å– ajaxgetfilteredrecommendations APIã€‚

        ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰«ææ‰€æœ‰è¯­è¨€ï¼Œå¤§å¹…æå‡é€Ÿåº¦ã€‚

        Returns:
            (all_unique_ids: set, page_name: str|None)
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import threading

        base_url = f"https://store.steampowered.com/curator/{curator_id}/ajaxgetfilteredrecommendations/"

        # æå‰æ£€æµ‹ä»£ç†å’Œç™»å½•çŠ¶æ€
        proxies = urllib.request.getproxies()
        has_proxy = bool(proxies.get('https') or proxies.get('http'))
        has_login = bool(re.search(r'steamLoginSecure=', cookies))
        can_adult_scan = has_proxy and has_login
        if progress_callback:
            if can_adult_scan:
                progress_callback(0, 0, "å‡†å¤‡æ‰«æ", "ğŸ” æ£€æµ‹åˆ°ä»£ç†+ç™»å½•æ€ï¼Œå°†é¢å¤–æ‰«ææˆäººæ¸¸æˆ")
            elif not has_proxy:
                progress_callback(0, 0, "å‡†å¤‡æ‰«æ", "ğŸ’¡ æœªæ£€æµ‹åˆ°ä»£ç†ï¼Œæˆäººæ¸¸æˆå¯èƒ½ä¸å®Œæ•´")

        all_unique_ids = set()
        _lock = threading.Lock()
        # å…±äº«è¿›åº¦çŠ¶æ€ï¼ˆä¾› progress_callback æ±‡æ€»æ˜¾ç¤ºï¼‰
        _lang_status = {}  # lang_display -> "çŠ¶æ€æ–‡å­—"
        _max_total = [0]

        def _report_progress():
            """æ±‡æ€»æ‰€æœ‰è¯­è¨€çš„è¿›åº¦å¹¶å›è°ƒ"""
            if not progress_callback:
                return
            with _lock:
                total = len(all_unique_ids)
                mt = _max_total[0]
                lines = [f"  {s}" for s in _lang_status.values() if s]
            detail = "\n".join(lines) if lines else ""
            progress_callback(total, mt, f"å·²è·å– {total} ä¸ª", detail)

        def _fetch_lang(lang_idx, lang_code, accept_lang, lang_display, country_code="US"):
            """å•ä¸ªè¯­è¨€+åŒºåŸŸçš„å®Œæ•´åˆ†é¡µæŠ“å–ï¼ˆåœ¨çº¿ç¨‹ä¸­è¿è¡Œï¼‰"""
            import re as _re
            # ä¸å‘é€ steamLoginSecureï¼Œå¦åˆ™æœåŠ¡å™¨ä¼šç”¨è´¦å·æ³¨å†ŒåŒºåŸŸè¦†ç›– cc å‚æ•°
            base_cookies = _re.sub(r'steamLoginSecure=[^;]*;?\s*', '', cookies)
            region_cookies = base_cookies.replace("steamCountry=US%7C0", f"steamCountry={country_code}%7C0")
            headers_api = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'application/json, text/javascript, */*; q=0.01',
                'Accept-Language': accept_lang,
                'X-Requested-With': 'XMLHttpRequest',
                'Referer': page_url,
                'Cookie': region_cookies,
            }

            local_ids = set()
            local_name = None
            start = 0
            count = 100
            total_count = None
            lang_page = 0

            with _lock:
                _lang_status[lang_display] = f"ğŸŒ {lang_display} æ­£åœ¨è¿æ¥..."
            _report_progress()

            while True:
                url = f"{base_url}?start={start}&count={count}&l={lang_code}&cc={country_code}"
                lang_page += 1

                try:
                    req = urllib.request.Request(url, headers=headers_api)
                    with urllib.request.urlopen(req, timeout=30, context=self.ssl_context) as resp:
                        data = json.loads(resp.read().decode('utf-8'))

                    if not data.get('success'):
                        break

                    if total_count is None:
                        total_count = int(data.get('total_count', 0))
                        if total_count == 0:
                            break
                        with _lock:
                            if total_count > _max_total[0]:
                                _max_total[0] = total_count

                    html_chunk = data.get('results_html', '')
                    new_in_page = 0
                    if html_chunk:
                        chunk_ids = re.findall(r'data-ds-appid="(\d+)"', html_chunk)
                        for aid in chunk_ids:
                            aid_int = int(aid)
                            if aid_int not in local_ids:
                                new_in_page += 1
                            local_ids.add(aid_int)

                        if local_name is None:
                            name_match = re.search(r'class="curator_name"[^>]*>.*?<a[^>]*>(.*?)</a>', html_chunk, re.S)
                            if name_match:
                                local_name = re.sub(r'<[^>]+>', '', name_match.group(1)).strip()

                    total_pages = (total_count + count - 1) // count if total_count else "?"
                    with _lock:
                        all_unique_ids.update(local_ids)
                        _lang_status[lang_display] = (
                            f"ğŸŒ {lang_display} ç¬¬{lang_page}/{total_pages}é¡µ"
                            f"ï¼ˆ+{new_in_page}ï¼‰")
                    _report_progress()

                    start += count
                    if start >= total_count or not html_chunk:
                        break

                    time.sleep(0.05)

                except Exception:
                    break

            # æœ€ç»ˆåˆå¹¶
            with _lock:
                all_unique_ids.update(local_ids)
                _lang_status[lang_display] = f"âœ… {lang_display} å®Œæˆï¼ˆ{len(local_ids)} ä¸ªï¼‰"
            _report_progress()

            return local_ids, local_name

        # å¹¶è¡Œå¯åŠ¨æ‰€æœ‰è¯­è¨€ + ä»£ç†æ‰«æ
        workers = len(self._LANG_CONFIGS) + (1 if can_adult_scan else 0)
        with ThreadPoolExecutor(max_workers=workers) as pool:
            futures = {
                pool.submit(_fetch_lang, idx, lc, al, ld, cc): ld
                for idx, (lc, al, ld, cc) in enumerate(self._LANG_CONFIGS)
            }
            proxy_future = None
            if can_adult_scan:
                proxy_future = pool.submit(self._proxy_adult_scan, curator_id, cookies, progress_callback)

            for future in as_completed(futures):
                try:
                    local_ids, local_name = future.result()
                    if local_name and page_name is None:
                        page_name = local_name
                except Exception:
                    pass

            if proxy_future:
                try:
                    proxy_ids = proxy_future.result()
                    if proxy_ids:
                        adult_new = len(proxy_ids - all_unique_ids)
                        all_unique_ids.update(proxy_ids)
                        if adult_new > 0 and progress_callback:
                            progress_callback(
                                len(all_unique_ids), len(all_unique_ids),
                                f"å·²è·å– {len(all_unique_ids)} ä¸ª",
                                f"ğŸ” ä»£ç†æ‰«æå‘ç° {adult_new} ä¸ªæ–°æ¸¸æˆ")
                except Exception:
                    pass

        if progress_callback:
            progress_callback(
                len(all_unique_ids),
                _max_total[0] if _max_total[0] else len(all_unique_ids),
                f"å·²è·å– {len(all_unique_ids)} ä¸ª",
                f"âœ… å…¨éƒ¨è¯­è¨€æ‰«æå®Œæˆ â€” å…± {len(all_unique_ids)} ä¸ªå”¯ä¸€æ¸¸æˆ"
            )

        return all_unique_ids, page_name

    def fetch_generic_list(self, page_type, identifier, type_name_cn, cookies, has_login, progress_callback=None):
        """é€šè¿‡é€šç”¨æ–¹å¼æŠ“å–å‘è¡Œå•†/å¼€å‘å•†/ç³»åˆ—ç­‰é¡µé¢çš„æ¸¸æˆåˆ—è¡¨"""
        from urllib.parse import unquote

        base_url = f"https://store.steampowered.com/{page_type}/{identifier}"

        # HTML é¡µé¢è·å–åªç”¨åŸºç¡€ cookieï¼Œé¿å…ç™»å½• cookie å¯¼è‡´é‡å®šå‘å¾ªç¯
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Cookie': self._BASE_COOKIES,
        }

        all_unique_ids = set()
        page_name = None

        if progress_callback:
            progress_callback(0, 0, "æ­£åœ¨è·å–é¡µé¢...", f"æ­£åœ¨è¿æ¥ {page_type}/{identifier} ...")

        try:
            req = urllib.request.Request(base_url, headers=headers)
            with urllib.request.urlopen(req, timeout=30, context=self.ssl_context) as resp:
                html_content = resp.read().decode('utf-8')

            name_patterns = [
                r'<div class="curator_name"[^>]*>.*?<a[^>]*>(.*?)</a>',
                r'<div class="page_title_area[^"]*"[^>]*>.*?<span[^>]*>(.*?)</span>',
                r'<h2 class="pageheader">(.*?)</h2>',
                r'<title>([^<]+?)(?:\s*[-â€“â€”]\s*Steam|\s*on Steam)?</title>',
            ]

            for pattern in name_patterns:
                match = re.search(pattern, html_content, re.S | re.I)
                if match:
                    extracted_name = match.group(1).strip()
                    extracted_name = re.sub(r'<[^>]+>', '', extracted_name)
                    extracted_name = extracted_name.replace('&amp;', '&').replace('&quot;', '"')
                    if extracted_name and len(extracted_name) < 100:
                        page_name = extracted_name
                        break

            if not page_name:
                page_name = unquote(identifier).replace('%20', ' ').replace('+', ' ')

            ids = self.extract_ids_from_html(html_content)
            for aid in ids:
                all_unique_ids.add(aid)

            if progress_callback:
                progress_callback(len(all_unique_ids), len(all_unique_ids), "å·²è·å–ä¸»é¡µé¢",
                                  f"ğŸ“„ ä¸»é¡µé¢æå–äº† {len(ids)} ä¸ªæ¸¸æˆï¼Œæ­£åœ¨æ£€æŸ¥åˆ†é¡µ...")

            page = 2
            while True:
                ajax_url = f"{base_url}?page={page}"
                try:
                    if progress_callback:
                        progress_callback(len(all_unique_ids), len(all_unique_ids), f"æ­£åœ¨è·å–ç¬¬ {page} é¡µ",
                                          f"ğŸ“„ æ­£åœ¨åŠ è½½ç¬¬ {page} é¡µ...")

                    req_page = urllib.request.Request(ajax_url, headers=headers)
                    with urllib.request.urlopen(req_page, timeout=15, context=self.ssl_context) as resp_page:
                        page_html = resp_page.read().decode('utf-8')

                    page_ids = self.extract_ids_from_html(page_html)
                    if not page_ids or all(aid in all_unique_ids for aid in page_ids):
                        break

                    new_count = sum(1 for aid in page_ids if aid not in all_unique_ids)
                    for aid in page_ids:
                        all_unique_ids.add(aid)

                    if progress_callback:
                        progress_callback(len(all_unique_ids), len(all_unique_ids), f"å·²è·å–ç¬¬ {page} é¡µ",
                                          f"ğŸ“„ ç¬¬ {page} é¡µæ–°å¢ {new_count} ä¸ªæ¸¸æˆï¼Œå½“å‰å…± {len(all_unique_ids)} ä¸ª")

                    page += 1
                    time.sleep(0.3)

                    if page > 50:
                        break

                except Exception:
                    break

        except urllib.error.HTTPError as e:
            return [], None, f"HTTP é”™è¯¯ {e.code}ï¼šæ— æ³•è®¿é—®è¯¥é¡µé¢ã€‚", has_login
        except Exception as e:
            return [], None, f"è·å–å¤±è´¥ï¼š{str(e)}", has_login

        if not all_unique_ids:
            return [], None, f"è¯¥{type_name_cn}é¡µé¢æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¸¸æˆã€‚", has_login

        unique_ids = list(all_unique_ids)
        display_name = f"{type_name_cn}ï¼š{page_name}"

        return unique_ids, display_name, None, has_login

    @staticmethod
    def extract_ids_from_steamdb_html(html_text):
        """ä» SteamDB é¡µé¢æºä»£ç ä¸­æå– AppID"""
        tbody_match = re.search(r'<tbody.*?>(.*?)</tbody>', html_text, re.DOTALL)
        if not tbody_match:
            return []
        return [int(aid) for aid in re.findall(r'data-appid="(\d+)"', tbody_match.group(1))]

    def perform_incremental_update(self, data, target_entry, new_ids_from_src, raw_name, create_aux=True):
        """æ ¸å¿ƒå¢é‡æ›´æ–°é€»è¾‘ï¼šä¸»æ”¶è—å¤¹è¿½åŠ  + å¯é€‰ç”Ÿæˆä¸¤ä¸ªå·®å¼‚è¾…åŠ©æ”¶è—å¤¹

        Args:
            create_aux: æ˜¯å¦åˆ›å»º"æ¯”æ—§ç‰ˆå¤šçš„"/"æ¯”æ—§ç‰ˆå°‘çš„"è¾…åŠ©æ”¶è—å¤¹

        Returns:
            (added_count, removed_count, total_count, is_updated)
            å¦‚æœæ²¡æœ‰æ–°å¢ä»»ä½•æ¸¸æˆï¼Œis_updated ä¸º Falseï¼Œæ­¤æ—¶ä¸ä¼šåšä»»ä½•ä¿®æ”¹
        """
        val_obj = json.loads(target_entry[1]['value'])
        old_ids = val_obj.get("added", [])

        old_set = set(old_ids)
        src_set = set(new_ids_from_src)

        added_list = [aid for aid in new_ids_from_src if aid not in old_set]
        removed_list = [aid for aid in old_ids if aid not in src_set]

        # å¦‚æœæ²¡æœ‰æ–°å¢ä»»ä½•æ¸¸æˆï¼Œä¸åšä»»ä½•æ“ä½œ
        if not added_list:
            return 0, len(removed_list), len(old_ids), False

        # æœ‰æ–°å¢ï¼Œæ‰§è¡Œæ›´æ–°
        val_obj['added'] = old_ids + added_list
        clean_name = raw_name.replace(self.induce_suffix, "").strip()
        suffix = "" if self._cef_active else self.induce_suffix
        val_obj['name'] = f"{clean_name}{suffix}"
        target_entry[1]['value'] = json.dumps(val_obj, ensure_ascii=False, separators=(',', ':'))
        target_entry[1]['timestamp'] = int(time.time())
        target_entry[1]['version'] = self.next_version(data)
        target_entry[1].setdefault('conflictResolutionMethod', 'custom')
        target_entry[1].setdefault('strMethodId', 'union-collections')

        # ä¸»æ”¶è—å¤¹ CEF åŒæ­¥
        col_id = val_obj.get("id", "")
        if col_id:
            self.queue_cef_upsert(col_id, val_obj['name'], val_obj['added'], val_obj.get('removed', []))

        # åˆ›å»ºè¾…åŠ©æ”¶è—å¤¹ï¼ˆadd_static_collection å†…éƒ¨å·²è‡ªå¸¦ queueï¼‰
        if create_aux:
            self.add_static_collection(data, f"{clean_name} - æ¯”æ—§ç‰ˆå¤šçš„", added_list)
            if removed_list:
                self.add_static_collection(data, f"{clean_name} - æ¯”æ—§ç‰ˆå°‘çš„", removed_list)

        return len(added_list), len(removed_list), len(val_obj['added']), True

    def perform_replace_update(self, data, target_entry, new_ids):
        """æ›¿æ¢å¼æ›´æ–°ï¼šç›´æ¥ç”¨æ–° ID åˆ—è¡¨æ›¿æ¢ç›®æ ‡æ”¶è—å¤¹çš„å†…å®¹

        Returns:
            (old_count, new_count)
        """
        val_obj = json.loads(target_entry[1]['value'])
        old_count = len(val_obj.get("added", []))

        val_obj['added'] = new_ids
        clean_name = val_obj.get('name', '').replace(self.induce_suffix, "").strip()
        suffix = "" if self._cef_active else self.induce_suffix
        val_obj['name'] = f"{clean_name}{suffix}"
        target_entry[1]['value'] = json.dumps(val_obj, ensure_ascii=False, separators=(',', ':'))
        target_entry[1]['timestamp'] = int(time.time())
        target_entry[1]['version'] = self.next_version(data)
        target_entry[1].setdefault('conflictResolutionMethod', 'custom')
        target_entry[1].setdefault('strMethodId', 'union-collections')

        # CEF åŒæ­¥é˜Ÿåˆ—
        col_id = val_obj.get("id", "")
        if col_id:
            self.queue_cef_upsert(col_id, val_obj['name'], new_ids, val_obj.get('removed', []))

        return old_count, len(new_ids)

    # --- æ”¶è—å¤¹å¯¼å‡º/å¯¼å…¥ï¼ˆä¸¤ç§æ ¼å¼ï¼‰ ---

    @staticmethod
    def export_collections_appid_list(collections):
        """æ ¼å¼ä¸€ï¼šå¯¼å‡ºé€‰ä¸­æ”¶è—å¤¹çš„å»é‡ AppID åˆ—è¡¨ï¼ˆä¸€è¡Œä¸€ä¸ªï¼‰
        åŠ¨æ€æ”¶è—å¤¹åªå¯¼å‡ºå…¶ added åˆ—è¡¨ã€‚"""
        seen = set()
        unique_ids = []
        for col in collections:
            for aid in col.get('added', []):
                if aid not in seen:
                    seen.add(aid)
                    unique_ids.append(aid)
        return unique_ids

    @staticmethod
    def export_collections_structured(collections):
        """æ ¼å¼äºŒï¼šå¯¼å‡ºé€‰ä¸­æ”¶è—å¤¹çš„å®Œæ•´ç»“æ„åŒ– JSON
        åŒ…å«åç§°ã€ç±»å‹ã€appidã€åŠ¨æ€é€»è¾‘ç­‰ã€‚"""
        export_data = {
            "format": "steam_collections_structured",
            "version": 1,
            "exported_at": datetime.now().isoformat(),
            "collections": []
        }
        for col in collections:
            entry = {
                "name": col.get("name", "æœªå‘½å"),
                "is_dynamic": col.get("is_dynamic", False),
                "added": col.get("added", []),
                "removed": col.get("removed", []),
            }
            if col.get("is_dynamic") and col.get("filterSpec"):
                entry["filterSpec"] = col["filterSpec"]
            export_data["collections"].append(entry)
        return export_data

    def import_collections_appid_list(self, file_path, data):
        """æ ¼å¼ä¸€ï¼šå¯¼å…¥ä¸€è¡Œä¸€ä¸ª AppID çš„åˆ—è¡¨æ–‡ä»¶ï¼Œåˆ›å»ºä¸€ä¸ªæ–°æ”¶è—å¤¹"""
        file_title = os.path.splitext(os.path.basename(file_path))[0]
        with open(file_path, 'r', encoding='utf-8') as f:
            app_ids = [int(line.strip()) for line in f if line.strip().isdigit()]
        if not app_ids:
            return None, "æ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„ AppIDã€‚"
        self.add_static_collection(data, file_title, app_ids)
        return len(app_ids), None

    def import_collections_structured(self, file_path, data):
        """æ ¼å¼äºŒï¼šå¯¼å…¥ç»“æ„åŒ– JSON æ–‡ä»¶ï¼Œè¿˜åŸå¤šä¸ªæ”¶è—å¤¹ï¼ˆå«åŠ¨æ€é€»è¾‘ï¼‰"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                import_data = json.load(f)
        except JSONDecodeError:
            return None, "æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼ã€‚"

        if import_data.get("format") != "steam_collections_structured":
            return None, "æ–‡ä»¶æ ¼å¼ä¸åŒ¹é…ï¼šç¼ºå°‘ format æ ‡è¯†ã€‚"

        imported_cols = import_data.get("collections", [])
        if not imported_cols:
            return None, "æ–‡ä»¶ä¸­æ²¡æœ‰æ”¶è—å¤¹æ•°æ®ã€‚"

        count = 0
        for col in imported_cols:
            name = col.get("name", "å¯¼å…¥çš„æ”¶è—å¤¹")
            is_dynamic = col.get("is_dynamic", False)
            added = col.get("added", [])
            removed = col.get("removed", [])

            if is_dynamic and "filterSpec" in col:
                # è¿˜åŸåŠ¨æ€æ”¶è—å¤¹
                col_id = f"uc-{secrets.token_hex(4)}"
                storage_key = f"user-collections.{col_id}"
                actual_name = name if self._cef_active else name + self.induce_suffix
                val_obj = {
                    "id": col_id,
                    "name": actual_name,
                    "added": added,
                    "removed": removed,
                    "filterSpec": col["filterSpec"]
                }
                new_entry = [storage_key, {
                    "key": storage_key,
                    "timestamp": int(time.time()),
                    "value": json.dumps(val_obj, ensure_ascii=False, separators=(',', ':')),
                    "version": self.next_version(data),
                    "conflictResolutionMethod": "custom",
                    "strMethodId": "union-collections"
                }]
                data.append(new_entry)
                self.queue_cef_upsert(col_id, actual_name, added, removed)
            else:
                # é™æ€æ”¶è—å¤¹
                self.add_static_collection(data, name.replace(self.induce_suffix, "").strip(), added)
            count += 1

        return count, None

    def add_dynamic_collection(self, data, name, friend_code):
        col_id = f"uc-{secrets.token_hex(4)}"
        storage_key = f"user-collections.{col_id}"
        actual_name = name if self._cef_active else name + self.induce_suffix
        filter_groups = [{"rgOptions": [], "bAcceptUnion": False} for _ in range(9)]
        filter_groups[0]["bAcceptUnion"] = True
        filter_groups[6]["rgOptions"] = [int(friend_code)]
        val_obj = {"id": col_id, "name": actual_name, "added": [], "removed": [],
                   "filterSpec": {"nFormatVersion": 2, "strSearchText": "", "filterGroups": filter_groups,
                                  "setSuggestions": {}}}
        new_entry = [storage_key, {"key": storage_key, "timestamp": int(time.time()),
                                   "value": json.dumps(val_obj, ensure_ascii=False, separators=(',', ':')),
                                   "version": self.next_version(data),
                                   "conflictResolutionMethod": "custom", "strMethodId": "union-collections"}]
        data.append(new_entry)
        self.queue_cef_upsert(col_id, actual_name, [])

    def fetch_steam250_ids(self, url, progress_callback=None):
        """ä» Steam250 é¡µé¢æå– AppID åˆ—è¡¨"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
        }

        if progress_callback:
            progress_callback(0, 0, "æ­£åœ¨è¿æ¥ Steam250...", "")

        try:
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req, timeout=20, context=self.ssl_context) as resp:
                html_content = resp.read().decode('utf-8')

            if progress_callback:
                progress_callback(0, 0, "æ­£åœ¨è§£æé¡µé¢...", "")

            raw_ids = re.findall(r'store\.steampowered\.com/app/(\d+)', html_content)

            unique_ids = []
            for aid in raw_ids:
                if aid not in unique_ids:
                    unique_ids.append(aid)

            app_ids = [int(aid) for aid in unique_ids[:250]]

            if not app_ids:
                return [], "æœªèƒ½ä»é¡µé¢æå–åˆ°ä»»ä½• AppIDã€‚é¡µé¢ç»“æ„å¯èƒ½å·²å˜åŒ–ã€‚"

            return app_ids, None

        except urllib.error.HTTPError as e:
            return [], f"HTTP é”™è¯¯ {e.code}ï¼šæ— æ³•è®¿é—® Steam250ã€‚"
        except urllib.error.URLError as e:
            return [], f"ç½‘ç»œé”™è¯¯ï¼š{str(e.reason)}"
        except Exception as e:
            return [], f"æå–å¤±è´¥ï¼š{str(e)}"



